{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd64a77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "* Machine learning (ML) helps in automatically finding complex and potentially useful patterns in data.\n",
    "* Preprocessing the data for ML involves both data engineering and feature engineering. Data engineering is the process of converting raw data into prepared data.\n",
    "* Feature engineering then tunes the prepared data to create the features expected by the ML model. \n",
    "* Pre-processing refers to the transformations applied to our data before feeding it to the algorithm. \n",
    "* Data Preprocessing is a technique that is used to convert the raw data into a clean data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d31bdd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imputation\n",
    "\n",
    "* Many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. \n",
    "* Imputation transformer for completing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01ce37da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Raw data before imputaton\n",
      " [[ 5.2  nan  3.5]\n",
      " [-2.   7.  -6.2]\n",
      " [-7.4 -5.4  nan]]\n",
      "\n",
      " Preprocessd data After imputation\n",
      " [[ 5.2   0.8   3.5 ]\n",
      " [-2.    7.   -6.2 ]\n",
      " [-7.4  -5.4  -1.35]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_data =  np.array([[5.2, np.nan, 3.5],\n",
    "                [-2.0,7.0,-6.2],\n",
    "                [-7.4,-5.4,np.nan]])\n",
    "\n",
    "# Taking care of missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "data = imp_mean.fit_transform(raw_data)\n",
    "print(\"\\n Raw data before imputaton\\n\",raw_data)\n",
    "print(\"\\n Preprocessd data After imputation\\n\",data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade7e23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Binarization\n",
    "\n",
    "* Binarization is the process of transforming data features of any entity into vectors of binary numbers to make classifier algorithms more efficient.\n",
    "* All the values above 2.0 becomes 1 and the remains values become 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbccc0cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binarized data:\n",
      " [[1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "BEFORE:\n",
      "Mean = [ 2.2  -0.85 -0.3 ]\n",
      "Std deviation = [3.75699348 5.69144094 3.03315018]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "data = np.array([[5.2, -3, 3.5],\n",
    "                [-2.0,7.0,-6.2],\n",
    "                [-7.4,-9.9,-5.4]])\n",
    "\n",
    "# Binarize data \n",
    "binarized = Binarizer(threshold=2.0).transform(data)\n",
    "print(\"\\nBinarized data:\\n\", binarized)\n",
    "\n",
    "# Print mean and standard deviation\n",
    "print(\"\\nBEFORE:\")\n",
    "print(\"Mean =\", data.mean(axis=0))\n",
    "print(\"Std deviation =\", data.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554eaa01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mean Removal\n",
    "\n",
    "* Removing the mean is a common preprocessing technique used in machine learning.\n",
    "* It helps to center each feature mean on zero  in order to remove bias from the featured in feature vectors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f29f8d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER:\n",
      "Mean = [-6.93889390e-17 -5.55111512e-17  5.55111512e-17]\n",
      "Std deviation = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data = np.array([[5.2, -3, 3.5],\n",
    "                [-2.0,7.0,-6.2],\n",
    "                [-7.4,-9.9,-5.4]])\n",
    "# Remove mean\n",
    "scaled = scale(data)\n",
    "print(\"\\nAFTER:\")\n",
    "print(\"Mean =\", scaled.mean(axis=0))\n",
    "print(\"Std deviation =\", scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c6f8c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MinMax Scaling\n",
    "\n",
    "* When the value of each feature varies between many random values, it becomes important to scale those features so that it is a  level playing field for the ML algorithm to train on.\n",
    "\n",
    "$$minmax = \\frac{x -min(x)}{max(x) -min(x)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59241803",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min max scaled data:\n",
      " [[1.         0.43396226 1.        ]\n",
      " [0.         1.         0.05882353]\n",
      " [0.92553191 0.59119497 0.82352941]\n",
      " [0.79787234 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array([[5.2, -3, 3.5],\n",
    "                [-2.0,7.0,-6.2],\n",
    "                [-7.4,-9.9,-5.4]])\n",
    "# Min max scaling\n",
    "scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_minmax = scaler_minmax.fit_transform(data)\n",
    "print(\"\\nMin max scaled data:\\n\", scaled_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617096cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normalization\n",
    "\n",
    "* Normalization modify the values in the feature vectors so that we can measure them on a common scale.\n",
    "The most common forms of normalization aim to modify the values so that they sum up to one(1).\n",
    "  * L1 normalization which refers to Least Absolute Deviations works by making sure that the sum of absolute values is 1 in each row.\n",
    "  * L2 normalization which refers to least squares works by making sure that the sum of squares is 1.\n",
    "* In general L1 normalization technique is considered more robust than L2 normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f2921a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 normalized data:\n",
      " [[ 0.4952381  -0.19047619  0.31428571]\n",
      " [-0.29370629  0.48951049 -0.21678322]\n",
      " [ 0.63380282  0.07042254  0.29577465]\n",
      " [ 0.21019108 -0.56687898 -0.22292994]]\n",
      "\n",
      "L2 normalized data:\n",
      " [[ 0.8030469  -0.30886419  0.50962592]\n",
      " [-0.4809826   0.80163767 -0.35501097]\n",
      " [ 0.90162439  0.10018049  0.42075805]\n",
      " [ 0.32618953 -0.87972328 -0.34595859]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "data = np.array([[5.2, -3, 3.5],\n",
    "                [-2.0,7.0,-6.2],\n",
    "                [-7.4,-9.9,-5.4]])\n",
    "# Normalize data\n",
    "l1_norm = normalize(data, norm='l1')\n",
    "l2_norm = normalize(data, norm='l2')\n",
    "print(\"\\nL1 normalized data:\\n\", l1_norm)\n",
    "print(\"\\nL2 normalized data:\\n\", l2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329dc1bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standardization\n",
    "\n",
    "* Feature Scaling means scaling features to the same scale\n",
    "* Standardization scales features to have a mean($\\mu$) of o and standard deviation ($\\alpha$) of 1:\n",
    " $$Standardization =\\frac{x-\\mu}{\\alpha}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7900375c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard Scaler:\n",
      " [[ 1.27872403 -0.14893866  1.41030554]\n",
      " [-0.11624764  1.29240322 -0.79614022]\n",
      " [-1.16247639 -1.14346456 -0.61416532]]\n",
      "\n",
      " Mean of Standard Scaler \n",
      " [7.40148683e-17 0.00000000e+00 1.11022302e-16]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "data = np.array([[5.2, -3, 3.5],\n",
    "                [-2.0,7.0,-6.2],\n",
    "                [-7.4,-9.9,-5.4]])\n",
    "# standarized data\n",
    "standard_scaler = std.fit_transform(data)\n",
    "print(\"\\nStandard Scaler:\\n\", standard_scaler)\n",
    "print(\"\\n Mean of Standard Scaler \\n\",standard_scaler.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d06c1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Label Encoder\n",
    "\n",
    "* When we perform classification, we usually deal with a lot of categorical labels. \n",
    "* These labels can be in the form of words, numbers or something else. However, the ML algorithms expect them to be numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8595a1e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label mapping:\n",
      "cloudy --> 0\n",
      "rainy --> 1\n",
      "sunny --> 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample input labels\n",
    "input_labels = ['rainy', 'cloudy', 'rainy', 'sunny', 'cloudy', 'sunny', 'sunny']\n",
    "\n",
    "# Create label encoder and fit the labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(input_labels)\n",
    "\n",
    "# Print the mapping \n",
    "print(\"\\nLabel mapping:\")\n",
    "for i, item in enumerate(encoder.classes_):\n",
    "    print(item, '-->', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1770e097",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labels = ['sunny', 'sunny', 'rainy']\n",
      "Encoded values = [2, 2, 1]\n",
      "\n",
      "Encoded values = [2, 0, 2, 1]\n",
      "Decoded labels = ['sunny', 'cloudy', 'sunny', 'rainy']\n"
     ]
    }
   ],
   "source": [
    "# Encode a set of labels using the encoder\n",
    "test_labels = ['sunny', 'sunny', 'rainy']\n",
    "encoded_values = encoder.transform(test_labels)\n",
    "print(\"\\nLabels =\", test_labels)\n",
    "print(\"Encoded values =\", list(encoded_values))\n",
    "\n",
    "# Decode a set of values using the encoder\n",
    "encoded_values = [2, 0, 2, 1]\n",
    "decoded_list = encoder.inverse_transform(encoded_values)\n",
    "print(\"\\nEncoded values =\", encoded_values)\n",
    "print(\"Decoded labels =\", list(decoded_list))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
